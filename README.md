Attention is all you need is the step by step implementation

and Final GPT is the end product

What is the project:\n

a GPT-like model, starting from a basic bigram language model and progressively incorporating advanced attention techniques. Our goal is to observe and document the improvements in performance as we enhance our model step by step.

We will be constructing our GPT model by focusing on the decoder section of the Transformer architecture, as outlined in the seminal paper "Attention is All You Need." This process will provide a comprehensive understanding of how each component contributes to the model's ability to generate coherent and contextually relevant text.

Throughout this notebook, we will:

Implement a simple bigram language model to establish a baseline.
Gradually integrate more sophisticated attention techniques, including multi-head attention and positional encoding.
Compare the performance of our models at each stage to highlight the impact of these enhancements.
By the end of this project, we aim to have a functional nanoGPT model that demonstrates the core principles of the Transformer architecture and showcases the power of attention mechanisms in natural language processing.
